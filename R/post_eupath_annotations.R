#' Create a series of POST requests which download all the annotation data for a species.
#'
#' The only way I have figured out how to download mass data from the eupathdb
#' is to ask for a raw dump of all available data using the GenesByGeneType
#' WADL.  Therefore, this function iterates over the various sequence types that
#' I have noticed at the eupathdb and does that for each type.
#'
#' @param entry Eupathdb annotation entry.
#' @param build_dir Location to dump the resulting data.
#' @param overwrite Overwrite existing data if it exists?
post_eupath_annotations <- function(entry = NULL, overwrite = FALSE, build_dir = "EuPathDB") {
    if (is.null(entry)) {
        stop("  Need an entry from the eupathdb.")
    }

    ## Check for output rda directory and create it if necessary
    rdadir <- file.path(build_dir, "rda")
    if (!file.exists(build_dir)) {
        created <- dir.create(build_dir, recursive = TRUE)
    }

    ## Look for an existing savefile and load if we are not overwriting existing data.
    savefile <- file.path(build_dir, glue::glue("{entry[['Genome']]}_annotations.rda"))
    if (file.exists(savefile)) {
        if (isTRUE(overwrite)) {
            removed <- file.remove(savefile)
        } else {
            message("  Delete the file ", savefile, " to regenerate.")
            result <- new.env()
            load(savefile, envir = result)
            result <- result[["result"]]
            return(result)
        }
    }

    ## query body as a structured list
    ## This list was generated by going to:
    ## view-source:http://tritrypdb.org/webservices/GeneQuestions/GenesByMolecularWeight.wadl
    ## scrolling down to the 'o-fields' section, and writing down the most likely
    ## useful column names.
    ## I later came through and wrote a function function to automagically populate this list.
    species <- entry[["TaxonUnmodified"]]
    webservice <- tolower(entry[["DataProvider"]])
    ## Use a query to find what annotation types are available: protein coding vs. rRNA vs. etc...
    types <- get_eupath_gene_types(webservice = webservice)
    result <- data.frame()

    ## Excepting schistodb, all the services are .orgs which is a .net.
    tld <- "org"
    if (webservice == "schistodb") {
        tld <- "net"
    }
    ## Finalize the URL to query using the webservice, tld, etc.
    service_directory <- prefix_map(webservice)
    ## download_json <- glue::glue("{build_dir}/{species_filename}.json")

    base_url <- glue::glue("https://{webservice}.{tld}/{service_directory}/service/record-types/transcript/searches/GenesByTaxon/reports/standard")
    wanted_columns <- get_semantic_columns(webservice = webservice)
    ##wanted_columns <- c("primary_key", "wdk_weight", "has_missing_transcripts", "gene_name",
    ##                    "gene_source_id", "gene_previous_ids", "gene_product", "transcript_product",
    ##                    "gene_exon_count", "exon_count", "gene_transcript_count",
    ##                    "three_prime_utr_length", "five_prime_utr_length", "strand", "gene_type",
    ##                    "is_pseudo", "transcript_length", "gene_entrez_id", "uniprot_id",
    ##                    "chromosome", "gene_location_text", "location_text", "sequence_id",
    ##                    "organism", "gene_ortholog_number", "gene_orthomcl_name",
    ##                    "gene_paralog_number", "gene_hts_noncoding_snps",
    ##                    "gene_hts_nonsyn_syn_ratio", "gene_hts_nonsynonymous_snps",
    ##                    "gene_hts_stop_codon_snps", "gene_hts_synonymous_snps",
    ##                    "gene_total_hts_snps", "cds", "transcript_sequence", "protein_sequence",
    ##                    "protein_length", "cds_length", "molecular_weight", "isoelectric_point",
    ##                    "interpro_id", "interpro_description", "pfam_id", "pfam_description",
    ##                    "pirsf_id", "pirsf_description", "prositeprofiles_id",
    ##                    "prositeprofiles_description", "smart_id", "smart_description",
    ##                    "superfamily_id", "superfamily_description", "tigrfam_id",
    ##                    "tigrfam_description", "new_product_name", "tm_count", "signalp_peptide",
    ##                    "signalp_scores", "predicted_go_id_component", "predicted_go_component",
    ##                    "predicted_go_id_function", "predicted_go_function",
    ##                    "predicted_go_id_process", "predicted_go_process",
    ##                    "annotated_go_id_component", "annotated_go_component",
    ##                    "annotated_go_id_function", "annotated_go_function",
    ##                    "annotated_go_id_process", "annotated_go_process", "ec_numbers",
    ##                    "ec_numbers_derived")
    query_body <- list(
        "searchConfig" = list(
            "parameters" = list("organism" = jsonlite::unbox(species)),
            "wdkWeight" = jsonlite::unbox(10)),
        "reportConfig" = list(
            "attributes" = wanted_columns,
            "tables" = list()))
    post_json <- jsonlite::toJSON(query_body)
    result <- httr::POST(url = base_url, body = post_json,
                         httr::content_type("application/json"),
                         httr::timeout(1200))
  ## Test the result to see that we actually got data.
  if (result[["status_code"]] == "422") {
      warn(sprintf("API request failed for %s (code = 422): ", entry[["Taxon"]]))
      return(data.frame())
  } else if (result[["status_code"]] == "400") {
      ## likely due to bad formatConfig
      warn(sprintf("API Request failed for %s (code = 400): ", entry[["Taxon"]]))
  } else if (result[["status_code"]] == "404") {
      warn(sprintf("API Request failed for %s (code = 404): ", entry[["Taxon"]]))
  } else if (result[["status_code"]] != "200") {
      warn(sprintf("API Request failed for %s (code = %d): ",
                   entry$Taxon, result[["status_code"]]))
      return(data.frame())
  } else if (length(result[["content"]]) < 100) {
      warn("Very small amount of content returned for :", entry[["Taxon"]])
  }
  cont <- httr::content(result, encoding = "UTF-8", as = "text")
    result <- try(jsonlite::fromJSON(cont, flatten = TRUE))
    if (class(result)[1] == "try-error") {
        stop("There was a parsing failure when reading the metadata.")
    }
    ## Every record contains and id, some fields, and tables.
    records <- result[["records"]]
    colnames(records) <- gsub(pattern = "^attributes\\.", replacement = "", x = colnames(records))
    records <- expand_list_columns(records)

    ## Use a heuristic to figure out numeric columns and set them accordingly.
    cnames <- colnames(records)
    for (i in 1:length(cnames)) {
        cname <- cnames[i]
        column <- records[[cname]]
        idx <- !is.na(column)
        column <- column[idx]
        res <- suppressWarnings(!is.na(as.numeric(as.character(column))))
        if (sum(res) == length(column)) {
            message("Setting ", cname, " to numeric.")
            records[[cname]] <- as.numeric(records[[cname]])
        }
    }

    ## Change entries which say 'N/A' to the actual NA value
    na_idx <- records == "N/A" | records == "NA"
    false_idx <- is.na(records)
    na_idx[false_idx] <- FALSE
    records[na_idx] <- NA

    ## Hopefully the data is consistent now, so let us change the column names
    ## and send the NAs to a contextually sensible value that sqlite will not yell about
    ## e.g. if a column is numeric, set it to 0; if a column is a character, set it to ""
    colnames(records) <- paste0("annot_", colnames(records))
    for (col_num in 1:length(colnames(records))) {
        cname <- colnames(records)[col_num]
        na_idx <- is.na(records[[col_num]])
        if (is.character(records[[col_num]])) {
            records[na_idx, col_num] <- ""
        } else {
            records[na_idx, col_num] <- 0
        }
        ## Cast factor columns explicitly as factors
        test_col <- as.factor(records[[col_num]])
        test_levels <- length(levels(test_col))
        if (test_levels < 50) {
            message("Setting ", cname, " to a factor.")
            records[[col_num]] <- as.factor(records[[col_num]])
        }
    }

    ## orgdbs seem to like uppercase column names
    colnames(records) <- toupper(colnames(records))

    ## Get rid of duplicated entries
    dup_idx <- duplicated(records)
    if (sum(dup_idx) > 0) {
        message("  Dropped ", sum(dup_idx), " duplicated entries.")
    }
    records <- records[!dup_idx, ]

    message("  Saving ", savefile, " with ", nrow(records), " rows.")
    save(records, file = savefile)
    return(records)
}
